<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Media Description with Chrome Built-in API</title>
</head>
<body>
    <h1>Chrome Built-In Multimodal image and sound input</h1>
       Github for these gitpages at <a href="https://github.com/hpssjellis/my-examples-of-web-llm">https://github.com/hpssjellis/my-examples-of-web-llm</a><br>
      Main Demo index at <a href="https://hpssjellis.github.io/my-examples-of-web-llm/public/index.html">https://hpssjellis.github.io/my-examples-of-web-llm/public/index.html</a>


      <details open>   <!--  You could use local storage to hide this after the first use. I did not want to make the page extra confusing -->
            <summary>Help to setup Chrome flags if needed</summary>
        <p>
            This page demonstrates the core features of the Gemini Nano Prompt API (`LanguageModel` API) available in Chrome 138+.
            Ensure you have enabled the necessary flags. Copy the link below and paste it into your Chrome address bar:
            <input type="text" id="flagsLink" class="copy-input" size="45" value="chrome://flags#optimization-guide-on-device-model" readonly>
            <button class="copy-button" onclick="copyFlagsLink()">Copy - tab</button><br>
            Then set <code>#optimization-guide-on-device-model</code> (to Enabled BypassPrefRequirement)<br>

            Then search for <input type="text" size="15" value="gemini-nano"> and set all of them to enabled:
        </p>
        <p>
            <ol>
                <li><code>#prompt-api-for-gemini-nano</code> (set to Enabled)</li>
                <li><code>#prompt-api-for-gemini-nano-multimodal-input</code>(set to Enabled) </li>
                <li><code>#summarization-api-for-gemini-nano</code>(set to Enabled) </li>
                <li><code>#writer-api-for-gemini-nano</code>(set to Enabled) </li>
                <li><code>#rewriter-api-for-gemini-nano</code>(set to Enabled) </li>
                <li><code>#proofreader-api-for-gemini-nano</code>(set to Enabled) </li>
            </ol>
            The Gemini Nano model will download the first time you use it. That will be about 4.0 GB of download and will need
            about 20 GB saving space for the final folders. <br><br>
            Note: User Ctrl-Shift-i to show comments

        
        </p>
        </details>
    <p>Select an image or audio file, or use your webcam/mic to capture media, to describe it using the Language Model API.</p>

    <h3>Image Input</h3>
    <input type="file" id="myImageInput" accept="image/*" />
    <button onclick="myStartWebcam()">Start Webcam</button>
    <button id="myCaptureImageButton" onclick="myCaptureImage()" style="display: none;">Capture Image</button>

    <h3>Audio Input</h3>
    <input type="file" id="myAudioInput" accept="audio/*" />
    <button id="myStartRecordingButton" onclick="myStartAudioRecording()">Start Mic Recording</button>
    <button id="myStopRecordingButton" onclick="myStopAudioRecording()" style="display: none;">Stop Mic Recording</button>
    <audio id="myAudioPreview" controls style="display: none;"></audio>

    <hr>

    <button id="myDescribeButton" onclick="myStartDescription()">Describe Media</button>
    <button id="myStopButton" onclick="myStopDescription()" style="display: none;">Stop Description</button>

    <div id="myStatus">Status here</div>
    <h2>Description:</h2>
    <p id="myOutputText"></p>

    <img id="myImagePreview" alt="Image Preview" style="display: none; max-width: 100%; height: auto;" />
    <video id="myWebcamPreview" style="display: none; max-width: 100%; height: auto;"></video>
    <canvas id="myCanvas" style="display: none;"></canvas>
    <br>
    <hr>
    <br>

    <h3>Chrome AI Translator</h3>

  <p>
    Source: 
    <select id="mySourceLang">
      <option value="it">Italian</option>
      <option value="en">English</option>
      <option value="fr">French</option>
      <option value="es">Spanish</option>
      <option value="de">German</option>
      <option value="pt">Portuguese</option>
    </select>
    → Target: 
    <select id="myTargetLang">
      <option value="en">English</option>
      <option value="it">Italian</option>
      <option value="fr">French</option>
      <option value="es">Spanish</option>
      <option value="de">German</option>
      <option value="pt">Portuguese</option>
    </select>
  </p>

  <textarea id="myInput" rows="6" cols="70" placeholder="Type text here...">Oggi è davvero una bella giornata</textarea><br>

  <input type="button" value="Translate" onclick="myHandleTranslateClick()">
  <input type="button" value="Release Translator" onclick="myReleaseTranslator()">

  <p><b>Status:</b></p>
  <div id="myStatus" style="white-space:pre-wrap;"></div>

  <textarea id="myOutput" rows="6" cols="70" placeholder="Translation here..."></textarea><br>
<div>
        <label for="myVoiceSelect">Choose a voice:</label>
        <select id="myVoiceSelect"></select>
    </div>
    <br>
    
    <button id="mySpeakButton">Speak Text</button>
    
    <p id="myStatusMessage"></p>

    <script type="module">
        // Element variables
        const myImageInput = document.getElementById('myImageInput');
        const myImagePreview = document.getElementById('myImagePreview');
        const myAudioInput = document.getElementById('myAudioInput');
        const myAudioPreview = document.getElementById('myAudioPreview');
        const myOutputText = document.getElementById('myOutputText');
        const myStatus = document.getElementById('myStatus');
        const myWebcamPreview = document.getElementById('myWebcamPreview');
        const myCaptureImageButton = document.getElementById('myCaptureImageButton');
        const myStartRecordingButton = document.getElementById('myStartRecordingButton');
        const myStopRecordingButton = document.getElementById('myStopRecordingButton');
        const myDescribeButton = document.getElementById('myDescribeButton');
        const myStopButton = document.getElementById('myStopButton');
        const myCanvas = document.getElementById('myCanvas');

        // State variables
        let myImageBlob = null;
        let myAudioBlob = null;
        let myTimerId = null;
        let myAnalysisTimerId = null;
        let myAbortController = null;
        let myTimeoutId = null;
        let myMediaRecorder = null;
        let myAudioChunks = [];



        // Image file selection handler
        myImageInput.onchange = async (event) => {
            const myFile = event.target.files[0];
            if (myFile) {
                myImageBlob = myFile;
                myAudioBlob = null; // Clear other media
                myImagePreview.src = URL.createObjectURL(myFile);
                myImagePreview.style.display = 'block';
                myWebcamPreview.style.display = 'none';
                myAudioPreview.style.display = 'none';
                myCaptureImageButton.style.display = 'none';
                myOutputText.textContent = 'Image selected. Ready to describe...';
                myStatus.textContent = '';
            }
        };

        // Audio file selection handler
        myAudioInput.onchange = async (event) => {
            const myFile = event.target.files[0];
            if (myFile) {
                myAudioBlob = myFile;
                myImageBlob = null; // Clear other media
                myAudioPreview.src = URL.createObjectURL(myFile);
                myAudioPreview.style.display = 'block';
                myImagePreview.style.display = 'none';
                myWebcamPreview.style.display = 'none';
                myCaptureImageButton.style.display = 'none';
                myOutputText.textContent = 'Audio file selected. Ready to describe...';
                myStatus.textContent = '';
            }
        };

        // Start webcam functionality
        window.myStartWebcam = async () => {
            try {
                const myStream = await navigator.mediaDevices.getUserMedia({ video: true });
                myWebcamPreview.srcObject = myStream;
                myWebcamPreview.style.display = 'block';
                myWebcamPreview.play();
                myCaptureImageButton.style.display = 'block';
                myImageInput.style.display = 'none';
                myImagePreview.style.display = 'none';
                myAudioInput.style.display = 'none';
                myAudioPreview.style.display = 'none';
                myOutputText.textContent = 'Webcam active. Click "Capture Image" to continue.';
                myImageBlob = null;
                myAudioBlob = null;
            } catch (error) {
                myOutputText.textContent = 'Error starting webcam: ' + error.message;
            }
        };

        // Capture image from webcam
        window.myCaptureImage = () => {
            myCanvas.width = myWebcamPreview.videoWidth;
            myCanvas.height = myWebcamPreview.videoHeight;
            const myContext = myCanvas.getContext('2d');
            myContext.drawImage(myWebcamPreview, 0, 0, myCanvas.width, myCanvas.height);
            myCanvas.toBlob((myBlob) => {
                myImageBlob = myBlob;
                myAudioBlob = null;
                myImagePreview.src = URL.createObjectURL(myBlob);
                myImagePreview.style.display = 'block';
                myWebcamPreview.pause();
                myWebcamPreview.srcObject.getTracks().forEach(track => track.stop());
                myWebcamPreview.style.display = 'none';
                myCaptureImageButton.style.display = 'none';
                myImageInput.style.display = 'block';
                myOutputText.textContent = 'Image captured. Ready to describe...';
            }, 'image/jpeg');
        };

        // Start audio recording
        window.myStartAudioRecording = async () => {
            try {
                const myStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                myMediaRecorder = new MediaRecorder(myStream);
                myAudioChunks = [];
                myMediaRecorder.ondataavailable = (event) => {
                    myAudioChunks.push(event.data);
                };
                myMediaRecorder.onstop = () => {
                    myAudioBlob = new Blob(myAudioChunks, { type: 'audio/mpeg' });
                    myAudioPreview.src = URL.createObjectURL(myAudioBlob);
                    myAudioPreview.style.display = 'block';
                    myOutputText.textContent = 'Audio recorded. Ready to describe...';
                    myStatus.textContent = '';
                    myImageBlob = null;
                    myAudioInput.style.display = 'block';
                    myImageInput.style.display = 'block';
                };
                myMediaRecorder.start();
                myStatus.textContent = 'Recording audio...';
                myStartRecordingButton.style.display = 'none';
                myStopRecordingButton.style.display = 'block';
            } catch (error) {
                myOutputText.textContent = 'Error starting mic: ' + error.message;
            }
        };

        // Stop audio recording
        window.myStopAudioRecording = () => {
            if (myMediaRecorder && myMediaRecorder.state !== 'inactive') {
                myMediaRecorder.stop();
                myMediaRecorder.stream.getTracks().forEach(track => track.stop());
            }
            myStatus.textContent = 'Recording stopped.';
            myStartRecordingButton.style.display = 'block';
            myStopRecordingButton.style.display = 'none';
        };

        // Stop description process
        window.myStopDescription = () => {
            if (myAbortController) {
                myAbortController.abort();
                myAbortController = null;
            }
            clearInterval(myTimerId);
            clearInterval(myAnalysisTimerId);
            clearTimeout(myTimeoutId);
            myStatus.textContent = 'Process stopped.';
            myDescribeButton.disabled = false;
            myStopButton.style.display = 'none';
        };

        // Start the description process
        window.myStartDescription = async () => {
            if (!myImageBlob && !myAudioBlob) {
                myOutputText.textContent = 'Please select or capture media first.';
                return;
            }
            myDescribeButton.disabled = true;
            myStopButton.style.display = 'block';
            myAbortController = new AbortController();
            let mySeconds = 0;
            myOutputText.textContent = 'Model is being downloaded...';
            myTimerId = setInterval(() => {
                mySeconds++;
                myOutputText.textContent = `Model is being downloaded... ${mySeconds} seconds`;
            }, 1000);
            myTimeoutId = setTimeout(() => {
                if (myAbortController) {
                    myAbortController.abort();
                }
            }, 30000); // 30 second timeout

            try {
                const myExpectedInputs = myImageBlob ? [{ type: 'image' }] : [{ type: 'audio' }];
                // Log information about the AI model and its parameters
                console.log('Attempting to create a Language Model session with expected inputs:', myExpectedInputs);
                console.log('Note: The Language Model API does not expose parameters like topk or temperature.');

                const mySession = await LanguageModel.create({
                    expectedInputs: myExpectedInputs,
                });
                // Log information after the session is successfully created
                console.log('Language Model session created successfully. Ready for prompt streaming.');

                clearInterval(myTimerId);
                myOutputText.textContent = '';
                let myAnalysisSeconds = 0;
                myAnalysisTimerId = setInterval(() => {
                    myAnalysisSeconds++;
                    myStatus.textContent = `Analyzing... ${myAnalysisSeconds} seconds`;
                }, 1000);

                const myContentType = myImageBlob ? 'image' : 'audio';
                const myContentValue = myImageBlob || myAudioBlob;
                console.log('myContentType');
                console.log(myContentType);
                console.log('myContentValue');
                console.log(myContentValue);
                let myPromptValue = `Transcribe this ${myContentType} as completely as you can and after that describe it`;    // default sound
                if (myContentType === 'image' ){
                   myPromptValue = `Describe this ${myContentType} as completely as you can`;   // switch for image
                }
                

                // Log the prompt being sent to the model
                console.log(`Sending prompt to the model: "${myPromptValue}"`);

                const myStream = mySession.promptStreaming([
                    {
                        role: 'user',
                        content: [
                            {
                                type: myContentType,
                                value: myContentValue,
                            },
                            {
                                type: 'text',
                                value: myPromptValue,
                            },
                        ],
                    },
                ], { signal: myAbortController.signal }, { outputLanguage: 'en' }); // Added outputLanguage parameter

                for await (const myChunk of myStream) {
                    myOutputText.append(myChunk);
                }

                clearInterval(myAnalysisTimerId);
                clearTimeout(myTimeoutId);
                myStatus.textContent = 'Analysis complete.';
                // Log when the description is finished
                console.log('Description finished. Output is now on the page.');


                myString = myOutputText;
                myRegex = /(?:##\s*Transcription\s*of\s*Audio:\s*")(.+)(?:".*)/;
                myResult = myString.replace(myRegex, "$1");
                
                console.log(myResult);

            } catch (error) {
                clearInterval(myTimerId);
                clearInterval(myAnalysisTimerId);
                clearTimeout(myTimeoutId);
                if (error.name === 'AbortError') {
                    myStatus.textContent = 'Description process was stopped.';
                    console.error('The user aborted the process.');
                } else {
                    myStatus.textContent = 'An error occurred. Please check the console for details.';
                    console.error('An error occurred during the model interaction:', error);
                }
            } finally {
                myAbortController = null;
                myDescribeButton.disabled = false;
                myStopButton.style.display = 'none';
            }
        };


                /**
         * Function to copy the chrome://flags link to the clipboard and open a new tab.
         */
       window.copyFlagsLink = async () => {
            const flagsInput = document.getElementById('flagsLink');
            flagsInput.select();
            flagsInput.setSelectionRange(0, 99999);

            document.execCommand('copy');
            window.open('about:blank', '_blank');

           // myStatus.textContent =`Copied "chrome://flags" to clipboard and opened a new tab!`;
        }

    </script>
    <script>

    let myTranslator = null;
    let myTimerInterval = null;

    function mySetStatus(msg) {
      const d = document.getElementById('myStatus');
      d.textContent = msg;
    }

    function myStartTimer() {
      let sec = 0;
      mySetStatus("Working... 0s");
      myTimerInterval = setInterval(() => {
        sec++;
        mySetStatus(`Working... ${sec}s`);
      }, 1000);
    }

    function myStopTimer(msg) {
      if (myTimerInterval) clearInterval(myTimerInterval);
      myTimerInterval = null;
      mySetStatus(msg);
    }

    async function myCreateTranslator(source, target) {
      if (!('Translator' in self)) {
        mySetStatus('Translator API not available in this browser.');
        return null;
      }

      try {
        mySetStatus(`Checking availability for ${source} → ${target}...`);
        const avail = await Translator.availability({ sourceLanguage: source, targetLanguage: target });
        mySetStatus(`Availability: ${avail}`);
        if (avail === 'unavailable') return null;

        mySetStatus(`Creating translator for ${source} → ${target}...`);
        const translator = await Translator.create({
          sourceLanguage: source,
          targetLanguage: target,
          monitor(m) {
            m.addEventListener('downloadprogress', ev => {
              const loaded = ev.loaded ?? 'unknown';
              const total = ev.total ?? 'unknown';
              mySetStatus(`Downloading model: ${loaded} / ${total} bytes`);
            });
          }
        });
        return translator;
      } catch (err) {
        mySetStatus('Error creating translator: ' + (err.message || err));
        return null;
      }
    }

    async function myHandleTranslateClick() {
      const text = document.getElementById('myInput').value.trim();
      if (!text) { mySetStatus('Type some text first.'); return; }

      const source = document.getElementById('mySourceLang').value;
      const target = document.getElementById('myTargetLang').value;

      // If existing translator is for a different pair, release it
      if (myTranslator && (myTranslator.sourceLanguage !== source || myTranslator.targetLanguage !== target)) {
        await myReleaseTranslator();
      }

      if (!myTranslator) {
        myTranslator = await myCreateTranslator(source, target);
        if (!myTranslator) {
          mySetStatus('Translator could not be created.');
          return;
        }
      }

      try {
        myStartTimer();
        const result = await myTranslator.translate(text);
        myStopTimer('Translation complete.');
        document.getElementById('myOutput').value = result;
      //   document.getElementById('myTextInput').value = result;
      } catch (err) {
        myStopTimer('Error during translation: ' + (err.message || err));
        document.getElementById('myOutput').textContent = '';
      }
    }

    async function myReleaseTranslator() {
      if (myTranslator) {
        try {
          if (typeof myTranslator.destroy === 'function') {
            await myTranslator.destroy();
          } else if (typeof myTranslator.close === 'function') {
            await myTranslator.close();
          }
        } catch (err) {
          mySetStatus('Error releasing translator: ' + (err.message || err));
        }
        myTranslator = null;
        mySetStatus('Translator released.');
      } else {
        mySetStatus('No translator to release.');
      }
    }


      // to speak 
      
      // Use descriptive names and camelCase
      const myTextInput = document.getElementById("myOutput");
      const mySpeakButton = document.getElementById("mySpeakButton");
      const myStatusMessage = document.getElementById("myStatusMessage");
      const myVoiceSelect = document.getElementById("myVoiceSelect");

      // Function to populate the voice dropdown
      const myPopulateVoiceList = () => {
          const myVoices = window.speechSynthesis.getVoices();
          // Clear the voice select dropdown first
          myVoiceSelect.innerHTML = '';
          
          myVoices.forEach(voice => {
              const option = document.createElement('option');
              option.textContent = voice.name + ' (' + voice.lang + ')';
              option.setAttribute('data-lang', voice.lang);
              option.setAttribute('data-name', voice.name);
              option.value = voice.name;
              myVoiceSelect.appendChild(option);
          });
      };

      // Listen for when the browser loads the voices
      // This is a necessary step because voices are loaded asynchronously
      window.speechSynthesis.addEventListener('voiceschanged', myPopulateVoiceList);

      // Initial population if voices are already loaded
      myPopulateVoiceList();

      // Define the function
      const mySpeakFunction = () => {
          if ('speechSynthesis' in window) {
              const myTextToSpeak = myTextInput.value;
              const mySelectedVoiceName = myVoiceSelect.value;
              const mySelectedVoice = window.speechSynthesis.getVoices().find(voice => voice.name === mySelectedVoiceName);

              if (myTextToSpeak.trim() === '') {
                  myStatusMessage.textContent = 'Please enter some text to speak.';
                  return;
              }

              const myUtterance = new SpeechSynthesisUtterance(myTextToSpeak);
              myUtterance.voice = mySelectedVoice;

              // Set a simple status message while speaking
              myStatusMessage.textContent = 'Speaking...';
              
              myUtterance.onend = () => {
                  myStatusMessage.textContent = 'Finished speaking.';
              };

              myUtterance.onerror = (event) => {
                  myStatusMessage.textContent = 'An error occurred: ' + event.error;
              };

              window.speechSynthesis.speak(myUtterance);

          } else {
              myStatusMessage.textContent = 'Sorry, your browser does not support the Web Speech API.';
          }
      };

      // Static link to the function on the button
      mySpeakButton.onclick = mySpeakFunction;
      
  </script>













  
</body>
</html>
